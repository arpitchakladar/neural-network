{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dc077af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "h, w = 28, 28\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return np.exp(x)/(1 + np.exp(x))"
   ]
  },
  {
   "attachments": {
    "images.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "e639ca62",
   "metadata": {},
   "source": [
    "## Structure\n",
    "### Input layer\n",
    "Each pixel in the image corresponds to a input node (i.e. for 28x28 image there a total of 784 input nodes).\n",
    "\n",
    "### Output layer\n",
    "Each output node will correspond to the digits (0-9). The node with the maximum value will be taken as the answer.\n",
    "\n",
    "### Hidden layer\n",
    "The most important so far. There can be an arbitrary number of hidden layers depending upon the required complexity. Here we will use 2.\n",
    "\n",
    "![images.png](attachment:images.png)\n",
    "\n",
    "---\n",
    "\n",
    "## How are we going to do it?\n",
    "Each node is connected with each node in the layer before it, each one of these connections contains a weight and bias associated with it. Each node results in one singular floating point value which is calculated by $\\sigma(\\sum_{i=1}^{n} n_iw_i + b)$. Where $n$ is the total number of nodes in the previous layer, $n_i$ is the node in the previous layer, $w_i$ is the weight corresponding to it, $b$ is the bias and, $\\sigma(x) = \\frac{1}{1+e^{-x}}$ is the sigmoid function (used to normalize the values between 0 and 1). However the calculation is quite tedious. So we can simplify the contents of an entire layer using just a single matrix, and the output of a single layer as a single vector. Then we can simplity the entire process of calculating the value of each node into a just a few vector matrix multiplication and addition.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc} \n",
    "w_{1,1} & w_{1,2} & w_{1,3} & w_{1,4}\\\\\n",
    "w_{2,1} & w_{2,2} & w_{2,3} & w_{2,4}\\\\\n",
    "w_{3,1} & w_{3,2} & w_{3,3} & w_{3,4}\\\\\n",
    "w_{4,1} & w_{4,2} & w_{4,3} & w_{4,4}\\\\\n",
    "w_{5,1} & w_{5,2} & w_{5,3} & w_{5,4}\\\\\n",
    "w_{6,1} & w_{6,2} & w_{6,3} & w_{6,4}\\\\\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{cc} \n",
    "n_1\\\\ \n",
    "n_2\\\\\n",
    "n_3\\\\\n",
    "n_4\n",
    "\\end{array}\\right]\n",
    "+\n",
    "\\left[\\begin{array}{cc} \n",
    "b_1\\\\ \n",
    "b_2\\\\\n",
    "b_3\\\\\n",
    "b_4\\\\\n",
    "b_5\\\\\n",
    "b_6\n",
    "\\end{array}\\right]\n",
    "=\n",
    "\\left[\\begin{array}{cc} \n",
    "\\sum_{i=1}^{4} n_iw_{1,i} + b_1\\\\ \n",
    "\\sum_{i=1}^{4} n_iw_{2,i} + b_2\\\\\n",
    "\\sum_{i=1}^{4} n_iw_{3,i} + b_3\\\\\n",
    "\\sum_{i=1}^{4} n_iw_{4,i} + b_4\\\\\n",
    "\\sum_{i=1}^{4} n_iw_{5,i} + b_5\\\\\n",
    "\\sum_{i=1}^{4} n_iw_{6,i} + b_6\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "487fc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # n is the number of nodes in the current layer and np is the number of nodes in the previous layer\n",
    "    def __init__(self, n, n_p):\n",
    "        self.weights = np.random.randn(n, n_p)\n",
    "        self.biases = np.random.randn(n, 1)\n",
    "\n",
    "    def __call__(self, l_p):\n",
    "        return np.add(np.matmul(self.weights, l_p), self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c1f2ce10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72152318],\n",
       "       [0.06083665]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_nodes = 3\n",
    "hidden_layer_nodes = [6, 6]\n",
    "output_layer_nodes = 2\n",
    "layers = []\n",
    "n_p = input_layer_nodes\n",
    "for n in hidden_layer_nodes:\n",
    "    layers.append(Layer(n, n_p))\n",
    "    n_p = n\n",
    "layers.append(Layer(output_layer_nodes, n_p))\n",
    "\n",
    "result = sigmoid(np.random.randn(3, 1))\n",
    "for i in range(0, len(layers)):\n",
    "    result = sigmoid(layers[i](result))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf1acf",
   "metadata": {},
   "source": [
    "## Now the hard part.\n",
    "Actually making the neural network improve. But first we need to find the cost of our neural network i.e the deviation from the expected value. The cost function takes all the weights and biases and input and gives 1 number as the output which is the **cost** of the neural network.\n",
    "\n",
    "$$\n",
    "C(w_1, w_2, w_3, ..., w_k, b_1, b_2, ..., b_l) = \\sum_{i=0}^{n} (n_i - e_i)^2\n",
    "$$\n",
    "\n",
    "where $n$ is the number of output nodes, $n_i$ is the value of the output node and $e_i$ is the expected value of the output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0b340619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4838739390910543"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = np.array([1,0]).transpose()\n",
    "difference = np.subtract(expected, result)\n",
    "cost = (difference * difference).sum()\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84820e7d",
   "metadata": {},
   "source": [
    "## What to do?\n",
    "Our goal is to minimize the value of this cost function. By a method called **back-propagation**. This involves finding the negative gradient of the $C(...)$ function or the $-\\Delta C(...)$ which will be a vector where each component will tell us the required magnitude by which we should increase or decrease a particular weight or bias to minimize the $C(...)$ function.\n",
    "\n",
    "## How to find $-\\Delta C(...)$?\n",
    "The output node's result $a^{(L)}$ is given by the following expression.\n",
    "$$a^{(L)} = \\sigma(z^{(L)})$$\n",
    "and $z^{(L)}$ is given by\n",
    "$$z^{(L)} = \\sum_{i=0}^{n^(L-1)}w^{(L)}_ia^{(L-1)}_i + b^{(L)}$$\n",
    "Now we can use the chain rule to find the partial derivitive of $C(...)$ with respect to a given weight.\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_i^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}_i}\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\frac{\\partial C}{\\partial a^{(L)}}\\\\\n",
    "\\Rightarrow \\frac{\\partial C}{\\partial w_i^{(L)}} = a^{(L-1)}_i\\sigma'(z^{L})2(a^{(L)} - e^{(L)})\n",
    "$$\n",
    "where $e^{(L)}$ is the expected value for the particular node.\n",
    "\n",
    "The bias is even simpler.\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial b^{(L)}}\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\frac{\\partial C}{\\partial a^{(L)}}\\\\\n",
    "\\Rightarrow \\frac{\\partial C}{\\partial b^{(L)}} = \\sigma'(z^{L})2(a^{(L)} - e^{(L)})\n",
    "$$\n",
    "\n",
    "Now the partial derivitive of $C(...)$ with respect to $a^{(L - 1)}$ will be.\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial a^{(L-1)}_i} = \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\frac{\\partial C}{\\partial a^{(L)}}\\\\\n",
    "\\Rightarrow \\frac{\\partial C}{\\partial a^{(L-1)}_i} = w^{(L)}_i\\sigma'(z^{L})2(a^{(L)} - e^{(L)})\n",
    "$$\n",
    "\n",
    "## Back-propagation\n",
    "How about finding the partial derivitive of a weight that is in the 2nd to last layer? Well we can use the chain rule again.\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_i^{(L-1)}} = \\frac{\\partial z^{(L-1)}}{\\partial w^{(L-1)}_i}\\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}}\\frac{\\partial C}{\\partial a^{(L-1)}}\\\\\n",
    "\\Rightarrow \\frac{\\partial C}{\\partial w_i^{(L-1)}} = a^{(L-2)}_i\\sigma'(z^{L})\\frac{\\partial C}{\\partial a^{(L-1)}}\n",
    "$$\n",
    "similarly for the bias\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^{(L-1)}} = \\frac{\\partial z^{(L-1)}}{\\partial b^{(L-1)}}\\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}}\\frac{\\partial C}{\\partial a^{(L-1)}}\\\\\n",
    "\\Rightarrow \\frac{\\partial C}{\\partial b^{(L-1)}} = \\sigma'(z^{L})\\frac{\\partial C}{\\partial a^{(L-1)}}\n",
    "$$\n",
    "$\\frac{\\partial C}{\\partial a^{(L-1)}}$ can be calculated from the next node. This process will continue till we reach the input layer. This is known as back propagation.\n",
    "\n",
    "## Using matrices\n",
    "For simplification we have been using matrices and so we are going to use that for working together with each individual layer instead of each node.\n",
    "\n",
    "### Calculating the weights in the output layer\n",
    "First we take the results in the 2nd to last layer and then tile them horijontally according to the number \n",
    "$$\n",
    "\\left[\\begin{array}{cc} \n",
    "a_1\\\\ \n",
    "a_2\\\\\n",
    "a_3\\\\\n",
    "a_4\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "Then we create a row matrix with each element equal to $\\frac{\\partial C}{\\partial z^{(L)}_i}$.\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^{(L)}}=\\left[\\begin{array}{cc} \n",
    "\\frac{\\partial C}{\\partial z^{(L)}_1} & \\frac{\\partial C}{\\partial z^{(L)}_2} & \\frac{\\partial C}{\\partial z^{(L)}_3} & \\frac{\\partial C}{\\partial z^{(L)}_4} & \\frac{\\partial C}{\\partial z^{(L)}_5} & \\frac{\\partial C}{\\partial z^{(L)}_6}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "Then the transpose of their product will be the required changes in the weights.\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w^{(L)}} =\n",
    "\\left( \n",
    "\\left[\\begin{array}{cc} \n",
    "a_1\\\\ \n",
    "a_2\\\\\n",
    "a_3\\\\\n",
    "a_4\n",
    "\\end{array}\\right]\n",
    "\\left(\\frac{\\partial C}{\\partial z^{(L)}}\\right)^T\n",
    "\\right)^T\n",
    "$$\n",
    "\n",
    "### Moving on to the next layer.\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^{(L-1)}} = \\left(\n",
    "\\left(w^{(L)}\\right)^T\n",
    "\\frac{\\partial C}{\\partial z^{(L)}}\n",
    "\\right)*\\left[\\begin{array}{cc} \n",
    "\\frac{\\partial a^{(L-1)}_1}{\\partial z^{(L-1)}_1}\\\\\n",
    "\\frac{\\partial a^{(L-1)}_2}{\\partial z^{(L-1)}_2}\\\\\n",
    "\\frac{\\partial a^{(L-1)}_3}{\\partial z^{(L-1)}_3}\\\\\n",
    "\\frac{\\partial a^{(L-1)}_4}{\\partial z^{(L-1)}_4}\\\\\n",
    "\\frac{\\partial a^{(L-1)}_5}{\\partial z^{(L-1)}_5}\\\\\n",
    "\\frac{\\partial a^{(L-1)}_6}{\\partial z^{(L-1)}_6}\n",
    "\\end{array}\\right]\\\\\n",
    "\\Rightarrow \\frac{\\partial C}{\\partial z^{(L-1)}} = \n",
    "\\left(\n",
    "\\left(w^{(L)}\\right)^T\n",
    "\\frac{\\partial C}{\\partial z^{(L)}}\n",
    "\\right)\n",
    "*\n",
    "\\left[\\begin{array}{cc} \n",
    "\\sigma'(z^{(L-1)}_1)\\\\\n",
    "\\sigma'(z^{(L-1)}_2)\\\\\n",
    "\\sigma'(z^{(L-1)}_3)\\\\\n",
    "\\sigma'(z^{(L-1)}_4)\\\\\n",
    "\\sigma'(z^{(L-1)}_5)\\\\\n",
    "\\sigma'(z^{(L-1)}_6)\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "Now the weights can be calculated using the above formula. Bias is the same as $\\frac{\\partial C}{\\partial z^{(L)}}$.\n",
    "$$\\frac{\\partial C}{\\partial b^{(L)}} = \\frac{\\partial C}{\\partial z^{(L)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "de3a760a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[-0.34552723],\n",
       "         [ 1.12195271],\n",
       "         [ 0.46074974],\n",
       "         [ 0.84867875],\n",
       "         [ 0.37810147],\n",
       "         [-1.44097349]]),\n",
       "  array([[0.41446747],\n",
       "         [0.75435074],\n",
       "         [0.61319202],\n",
       "         [0.70028991],\n",
       "         [0.59341512],\n",
       "         [0.19139464]])),\n",
       " (array([[ 2.33901043],\n",
       "         [ 2.59620954],\n",
       "         [-0.21935143],\n",
       "         [-1.09668743],\n",
       "         [ 0.70017799],\n",
       "         [-0.78693923]]),\n",
       "  array([[0.91205674],\n",
       "         [0.93061723],\n",
       "         [0.44538097],\n",
       "         [0.25036108],\n",
       "         [0.66822723],\n",
       "         [0.31282625]])),\n",
       " (array([[-2.16947495],\n",
       "         [ 2.66326927]]),\n",
       "  array([[0.10252533],\n",
       "         [0.93482414]]))]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_nodes = 3\n",
    "hidden_layer_nodes = [6, 6]\n",
    "output_layer_nodes = 2\n",
    "layers = []\n",
    "n_p = input_layer_nodes\n",
    "for n in hidden_layer_nodes:\n",
    "    layers.append(Layer(n, n_p))\n",
    "    n_p = n\n",
    "layers.append(Layer(output_layer_nodes, n_p))\n",
    "\n",
    "input_data = np.random.randn(3, 1)\n",
    "results = []\n",
    "result = (input_data, sigmoid(input_data))\n",
    "for i in range(0, len(layers)):\n",
    "    result = layers[i](result[1])\n",
    "    result = (result, sigmoid(result))\n",
    "    results.append(result)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67560e6c",
   "metadata": {},
   "source": [
    "Now we are storing the results of each individual layer as well as the value without the sigmoid function applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8ebf1eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[-0.00943375,  0.0895965 ],\n",
       "         [-0.08601681,  0.81693924]]),\n",
       "  array([[-0.09201389],\n",
       "         [ 0.87389617]])),\n",
       " (array([[ 0.79230972, -0.15914099,  0.07602403,  0.06986249,  0.27862471,\n",
       "           0.07886537],\n",
       "         [ 1.44204181, -0.28964425,  0.1383674 ,  0.1271531 ,  0.50711038,\n",
       "           0.14353877],\n",
       "         [ 1.17219813, -0.23544425,  0.11247524,  0.10335943,  0.41221679,\n",
       "           0.11667891],\n",
       "         [ 1.33869732, -0.26888679,  0.12845124,  0.11804062,  0.47076812,\n",
       "           0.13325201],\n",
       "         [ 1.13439195, -0.22785061,  0.10884765,  0.10002585,  0.39892181,\n",
       "           0.11291574],\n",
       "         [ 0.36587632, -0.07348884,  0.03510672,  0.03226141,  0.12866457,\n",
       "           0.0364188 ]]),\n",
       "  array([[ 1.91163304],\n",
       "         [-0.38396496],\n",
       "         [ 0.18342581],\n",
       "         [ 0.16855965],\n",
       "         [ 0.67224747],\n",
       "         [ 0.1902812 ]])),\n",
       " (array([[0.08103281, 0.07605038, 0.01742722, 0.08725859, 0.09885242,\n",
       "          0.01959582],\n",
       "         [0.73885572, 0.69342602, 0.15890111, 0.79562221, 0.90133459,\n",
       "          0.17867437]]),\n",
       "  array([[0.79036867],\n",
       "         [0.74177163],\n",
       "         [0.16997968],\n",
       "         [0.85109292],\n",
       "         [0.96417556],\n",
       "         [0.19113153]]))]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = np.array([[1,0]]).transpose()\n",
    "ngc = []\n",
    "(z, a) = results[len(results) - 1]\n",
    "q = dsigmoid(z) * np.subtract(a, expected)\n",
    "ngc.append((np.matmul(a, q.transpose()), q))\n",
    "q = np.matmul(layers[len(layers) - 1].weights.transpose(), q)\n",
    "for i in range(len(layers) - 2, -1, -1):\n",
    "    (z, _) = results[i]\n",
    "    (_, a) = results[i - 1]\n",
    "    q *= dsigmoid(z)\n",
    "    ngc.append((np.matmul(a, q.transpose()), q))\n",
    "    q = np.matmul(layers[i].weights.transpose(), q)\n",
    "ngc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17c78c",
   "metadata": {},
   "source": [
    "## Creating the final algorithm\n",
    "Now all that's left is to take the average of the $-\\Delta C(...)$ across all the training data and adding them to the matrices in the neural network. Usually the $-\\Delta C(...)$ is multiplied by a contant $k$, which makes the neural network more prone to changes by new training data but also makes it more likely to deviate from the local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c1e2d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = []\n",
    "        n_p = layers[0]\n",
    "        for i in range(1, len(layers)):\n",
    "            n = layers[i]\n",
    "            self.layers.append(Layer(n, n_p))\n",
    "            n_p = n\n",
    "            \n",
    "    def result(self, input_data):\n",
    "        result = sigmoid(input_data)\n",
    "        for layer in self.layers:\n",
    "            result = sigmoid(layer(result))\n",
    "        return result\n",
    "\n",
    "    def _results(self, input_data):\n",
    "        results = []\n",
    "        result = (input_data, sigmoid(input_data))\n",
    "        results.append(result)\n",
    "        for i in range(0, len(self.layers)):\n",
    "            result = self.layers[i](result[1])\n",
    "            result = (result, sigmoid(result))\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "    def train(self, train_data, train_labels):\n",
    "        ngc = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            ngc.append([np.zeros(layer.weights.shape), np.zeros(layer.biases.shape)])\n",
    "\n",
    "        for j in range(len(train_data)):\n",
    "            results = self._results(train_data[j])\n",
    "            (z, a) = results[len(results) - 1]\n",
    "            (_, a2) = results[len(results) - 2]\n",
    "            q = dsigmoid(z) * np.subtract(a, train_labels[j])\n",
    "            last_ngc = ngc[len(ngc) - 1]\n",
    "            last_ngc[0] += np.matmul(a2, q.transpose()).transpose()\n",
    "            last_ngc[1] += q\n",
    "            q = np.matmul(self.layers[len(self.layers) - 1].weights.transpose(), q)\n",
    "            for i in range(len(results) - 2, 0, -1):\n",
    "                (z, _) = results[i]\n",
    "                (_, a) = results[i - 1]\n",
    "                q *= dsigmoid(z)\n",
    "                current_ngc = ngc[i - 1]\n",
    "                current_ngc[0] += np.matmul(a, q.transpose()).transpose()\n",
    "                current_ngc[1] += q\n",
    "                q = np.matmul(self.layers[i - 1].weights.transpose(), q)\n",
    "\n",
    "        for i in range(len(layers)):\n",
    "            layer = self.layers[i]\n",
    "            current_ngc = ngc[i]\n",
    "            layer.weights += current_ngc[0]\n",
    "            layer.biases +=  current_ngc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b23ba",
   "metadata": {},
   "source": [
    "## Training\n",
    "Let's train our neural network on a 100 data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b8a59444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_598/2371750339.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = NeuralNetwork(w*h, 1000, 1000, 10)\n",
    "training_labels = []\n",
    "training_data = []\n",
    "for i in range(1000):\n",
    "    label = np.zeros((10, 1))\n",
    "    label[train_labels[i]][0] = 1\n",
    "    training_labels.append(label)\n",
    "    training_data.append(np.reshape(train_images[i], (h*w, 1)))\n",
    "neural_network.train(np.array(training_data), np.array(training_labels))\n",
    "\n",
    "testing_labels = []\n",
    "for i in range(1):\n",
    "    label = np.zeros((10, 1))\n",
    "    label[test_labels[i]][0] = 1\n",
    "    testing_labels.append(label)\n",
    "print(testing_labels[0])\n",
    "neural_network.result(test_images[0].reshape(h*w, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f8e17",
   "metadata": {},
   "source": [
    "A accuracy of the above neural network is quite bad. This can be improved by using a larger data set. But 1000 is already more that what jupyter notebook can comfortably handle. So let's leave it at that for now :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
